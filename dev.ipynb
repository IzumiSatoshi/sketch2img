{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from diffusers import UNet2DModel\n",
    "from diffusers import AutoencoderKL\n",
    "from diffusers import DDPMScheduler, DPMSolverMultistepScheduler, DDIMScheduler\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from diffusers.pipeline_utils import DiffusionPipeline\n",
    "from diffusers import DDPMPipeline\n",
    "from diffusers import StableDiffusionPipeline, StableDiffusionDepth2ImgPipeline\n",
    "from PIL import Image\n",
    "import json\n",
    "import create_init_unet\n",
    "import pathlib\n",
    "import copy\n",
    "import wandb\n",
    "from pipeline_ddpm_sketch2img import DDPMSketch2ImgPipeline\n",
    "from datasets import FashionMNISTDataset\n",
    "device = \"cuda\"\n",
    "VAE_SCALE_FACTOR = 0.18215"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = FashionMNISTDataset(\"./data/FashionMNIST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = DDPMScheduler.from_pretrained(\"./model/from_init_test/scheduler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[159, 156, 153,  ..., 153, 152, 157],\n",
      "        [156, 152, 154,  ..., 151, 150, 155],\n",
      "        [155, 154, 155,  ..., 152, 154, 155],\n",
      "        ...,\n",
      "        [122, 121, 116,  ...,  92,  88,  95],\n",
      "        [124, 114, 125,  ..., 101, 104,  92],\n",
      "        [122, 112, 116,  ..., 102, 107, 102]], dtype=torch.uint8)\n",
      "tensor([[159, 156, 153,  ..., 153, 152, 157],\n",
      "        [156, 152, 154,  ..., 151, 150, 155],\n",
      "        [155, 154, 155,  ..., 152, 154, 155],\n",
      "        ...,\n",
      "        [122, 121, 116,  ...,  92,  88,  95],\n",
      "        [124, 114, 125,  ..., 101, 104,  92],\n",
      "        [122, 112, 116,  ..., 102, 107, 102]], dtype=torch.int32)\n",
      "torch.int32\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'uint16'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mprint\u001b[39m(tesnsor[\u001b[39m0\u001b[39m])\n\u001b[0;32m      6\u001b[0m \u001b[39mprint\u001b[39m(tesnsor\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m----> 7\u001b[0m img \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mto_pil_image(tesnsor\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39;49muint16))\n\u001b[0;32m      8\u001b[0m img\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch' has no attribute 'uint16'"
     ]
    }
   ],
   "source": [
    "pil_list = [Image.open(\"./data/dog.png\"), Image.open(\"./data/dog.7.jpg\")]\n",
    "tesnsor = transforms.functional.pil_to_tensor(pil_list[0])\n",
    "print(tesnsor[0])\n",
    "tesnsor = tesnsor.int()\n",
    "print(tesnsor[0])\n",
    "print(tesnsor.dtype)\n",
    "img = transforms.functional.to_pil_image(tesnsor.to(torch.uint16))\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StableDiffusionDepth2ImgPipeline {\n",
      "  \"_class_name\": \"StableDiffusionDepth2ImgPipeline\",\n",
      "  \"_diffusers_version\": \"0.11.1\",\n",
      "  \"depth_estimator\": [\n",
      "    \"transformers\",\n",
      "    \"DPTForDepthEstimation\"\n",
      "  ],\n",
      "  \"feature_extractor\": [\n",
      "    \"transformers\",\n",
      "    \"DPTImageProcessor\"\n",
      "  ],\n",
      "  \"scheduler\": [\n",
      "    \"diffusers\",\n",
      "    \"PNDMScheduler\"\n",
      "  ],\n",
      "  \"text_encoder\": [\n",
      "    \"transformers\",\n",
      "    \"CLIPTextModel\"\n",
      "  ],\n",
      "  \"tokenizer\": [\n",
      "    \"transformers\",\n",
      "    \"CLIPTokenizer\"\n",
      "  ],\n",
      "  \"unet\": [\n",
      "    \"diffusers\",\n",
      "    \"UNet2DConditionModel\"\n",
      "  ],\n",
      "  \"vae\": [\n",
      "    \"diffusers\",\n",
      "    \"AutoencoderKL\"\n",
      "  ]\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mizumisatoshi05\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>e:\\Projects\\sketch2img\\wandb\\run-20230102_145624-3rk24lya</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/izumisatoshi05/ddpm_train_test/runs/3rk24lya\" target=\"_blank\">breezy-bush-5</a></strong> to <a href=\"https://wandb.ai/izumisatoshi05/ddpm_train_test\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:22<00:00,  2.24s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>█▃▃▃▄▄▂▅▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>0.03502</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">breezy-bush-5</strong>: <a href=\"https://wandb.ai/izumisatoshi05/ddpm_train_test/runs/3rk24lya\" target=\"_blank\">https://wandb.ai/izumisatoshi05/ddpm_train_test/runs/3rk24lya</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230102_145624-3rk24lya\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def train(dataset):\n",
    "    # define configs\n",
    "    save_path = \"./model/from_init_test_train_func\"\n",
    "    pretrained_model_name_or_path = \"./model/init_s2i_fmnist_5epochs\"\n",
    "    num_epochs = 1\n",
    "    batch_size = 64\n",
    "    lr = 1e-5\n",
    "    grad_accumulation_steps = 2\n",
    "    train_dataset_rate = 0.01\n",
    "    device = \"cuda\"\n",
    "    wandb_project_name = \"ddpm_train_test\"\n",
    "\n",
    "    # wandb initizalize\n",
    "    config = dict(\n",
    "        batch_size=batch_size,\n",
    "        lr=lr,\n",
    "        grad_accumulation_steps=grad_accumulation_steps,\n",
    "        num_epochs=num_epochs,\n",
    "        device=device,\n",
    "        train_dataset_rate=train_dataset_rate,\n",
    "    )\n",
    "    wandb.init(project=wandb_project_name, config=config)\n",
    "\n",
    "    # load pipe\n",
    "    pipe = DDPMPipeline.from_pretrained(\n",
    "        pretrained_model_name_or_path=pretrained_model_name_or_path\n",
    "    ).to(device)\n",
    "\n",
    "    # split dataset\n",
    "    dataset_len = int(len(dataset) * train_dataset_rate)\n",
    "    dataset, _ = torch.utils.data.random_split(\n",
    "        dataset, [dataset_len, len(dataset) - dataset_len]\n",
    "    )\n",
    "\n",
    "    # train\n",
    "    dataloader = DataLoader(dataset, batch_size, shuffle=True)\n",
    "    optimizer = torch.optim.AdamW(pipe.unet.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(epoch)\n",
    "        for step, (image, sketch) in enumerate(tqdm(dataloader)):\n",
    "            bs = image.shape[0]\n",
    "\n",
    "            # prepare valiables\n",
    "            image = image.to(device)\n",
    "            sketch = sketch.to(device)\n",
    "            noise = torch.randn_like(image).to(device)\n",
    "            timesteps = torch.randint(\n",
    "                0, pipe.scheduler.num_train_timesteps, (bs,), device=device\n",
    "            ).long()\n",
    "\n",
    "            # set up model input\n",
    "            noisy_image = pipe.scheduler.add_noise(image, noise, timesteps)\n",
    "            model_input = torch.cat([noisy_image, sketch], dim=1).to(device)\n",
    "\n",
    "            # prediction\n",
    "            noise_pred = pipe.unet(model_input, timesteps).sample\n",
    "\n",
    "            # backward\n",
    "            loss = F.mse_loss(noise_pred, noise)\n",
    "            loss.backward(loss)\n",
    "\n",
    "            # optimizer's step\n",
    "            if (step + 1) % grad_accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            # logging\n",
    "            wandb.log({\"loss\": loss.item()})\n",
    "    # save\n",
    "    pipe.save_pretrained(save_path)\n",
    "\n",
    "    # finish wandb\n",
    "    wandb.finish()\n",
    "train(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = DDPMSketch2ImgPipeline.from_pretrained(\"./model/from_init_test_train_func\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.5832e-06, 1.3508e-05, 2.9377e-05],\n",
       "        [1.8843e-05, 3.2196e-05, 3.0885e-05],\n",
       "        [3.6792e-05, 2.4276e-05, 2.9926e-05]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.unet.conv_in.weight[0][1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "79091f8df886e111703d2034356509b2014a606bf4ba6033bd359d74a62ece83"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
